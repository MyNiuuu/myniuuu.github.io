<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Muyao Niu (牛慕尧)</title>

    <meta name="author" content="Muyao Niu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Muyao Niu (牛慕尧)
                </p>
                <p>Hi, I'm Muyao Niu.
                  I am a 1st year Ph.D. student in the Department of Mechano-Informatics, the University of Tokyo (UTokyo). My supervisor is Prof. <a href="https://www.ai.u-tokyo.ac.jp/ja/members/yqzheng">Yinqiang Zheng</a> . I received my B.E. degree from Dalian University of Technology in 2022. My research interests include AIGC, Computational Photography, and 3D Vision.
                </p>
                <p style="text-align:center">
                  <a href="mailto:muyao.niu@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://github.com/MyNiuuu">Github</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=CHK1G40AAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  WeChat: MyNiuuu
                </p>
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="images/niumuyao.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/niumuyao.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected 5 Papers</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding-top: 10px; padding-bottom: 10px;width:25%;vertical-align:middle;text-align:center">
                <img src='images/mofa.png' style="max-width: 100%; width: 100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model
                <br>
                <em>
                  <strong>Muyao Niu</strong>,
                  Xiaodong Cun,
                  Xintao Wang,
                  Yong Zhang,
                  Ying Shan,
                  Yinqiang Zheng
                </em>
                <br>
                  <em>ECCV</em>, 2024
                <br>
                <a href="https://myniuuu.github.io/MOFA_Video/">project page</a> / 
                <a href="https://arxiv.org/abs/2405.20222">paper</a> / 
                <a href="https://github.com/MyNiuuu/MOFA-Video">code</a>
                <br>
                We introduce MOFA-Video to adapt motions from different domains to the frozen Video Diffusion Model. MOFA-Video can effectively animate a single image using various types of control signals, including trajectories, keypoint sequences, and their combinations.
              </td>
            </tr>


            <tr>
              <td style="padding-top: 10px; padding-bottom: 10px;width:25%;vertical-align:middle;text-align:center">
                <img src='images/Anicrafter.jpg' style="max-width: 100%; width: 100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models
                <br>
                <em>
                  <strong>Muyao Niu</strong>, 
                  Mingdeng Cao, 
                  Yifan Zhan, 
                  Qingtian Zhu, 
                  Mingze Ma, 
                  Jiancheng Zhao, 
                  Yanhong Zeng, 
                  Zhihang Zhong, 
                  Xiao Sun, 
                  Yinqiang Zheng
                </em>
                <br>
                  <em>arXiv</em>, 2025
                <br>
                project page / 
                <a href="https://arxiv.org/abs/2505.20255">paper</a> / 
                <a href="https://github.com/MyNiuuu/AniCrafter">code</a>
                <br>
                We leverage "3DGS Avatar + Background Video" as guidance for the video diffusion model to insert and animate anyone into any scene following given motion sequence.
              </td>
            </tr>	


            <tr>
              <td style="padding-top: 10px; padding-bottom: 10px;width:25%;vertical-align:middle;text-align:center">
                <img src='images/cvpr2023.png' style="max-width: 100%; width: 100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                Visibility Constrained Wide-band Illumination Spectrum Design for Seeing-in-the-Dark
                <br>
                <em>
                  <strong>Muyao Niu</strong>,
                  Zhuoxiao Li,
                  Zhihang Zhong,
                  Yinqiang Zheng
                </em>
                <br>
                  <em>CVPR</em>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Niu_Visibility_Constrained_Wide-Band_Illumination_Spectrum_Design_for_Seeing-in-the-Dark_CVPR_2023_paper.pdf">paper</a> / 
                <a href="https://github.com/MyNiuuu/VCSD">code</a>
                <br>
                We designed an optimal illumination spectrum in the VIS-NIR range by considering human vision constraints, which significantly improves translation performance. A fully differentiable model was proposed, which includes the imaging process, human visual perception, and the enhancement network.
              </td>
            </tr>


            <tr>
              <td style="padding-top: 10px; padding-bottom: 10px;width:25%;vertical-align:middle;text-align:center">
                <img src='images/RSNeRF.png' style="max-width: 100%; width: 100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                RS-NeRF: Neural Radiance Fields from Rolling Shutter Images
                <br>
                <em>
                  <strong>Muyao Niu</strong>,
                  Tong Chen,
                  Yifan Zhan,
                  Zhuoxiao Li,
                  Xiang Ji,
                  Yinqiang Zheng
                </em>
                <br>
                  <em>ECCV</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2407.10267">paper</a> / 
                <a href="https://github.com/MyNiuuu/RS-NeRF">code</a>
                <br>
                We improve NeRF to consider the RS distortions with two technologies: camera trajectory smoothness regularization and multi-sampling strategy.
              </td>
            </tr>
            
            <tr>
              <td style="padding-top: 10px; padding-bottom: 10px;width:25%;vertical-align:middle;text-align:center">
                <img src='images/MM2023.png' style="max-width: 100%; width: 100%;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  Physics-Based Adversarial Attack on Near-Infrared Human Detector for Nighttime Surveillance Camera Systems
                <br>
                <em>
                  <strong>Muyao Niu</strong>,
                  Zhuoxiao Li
                  Yifan Zhan,
                  Huy H. Nguyen,
                  Isao Echizen,
                  Yinqiang Zheng
                </em>
                <br>
                  <em>ACM MM</em>, 2023
                <br>
                <a href="https://dl.acm.org/doi/10.1145/3581783.3612082">paper</a> / 
                <a href="https://github.com/MyNiuuu/AdvNIR">code</a>
                <br>
                We introduced an innovative approach that passively manipulates the intensity distribution of NIR images and developed a 3D-aware, black-box attack algorithm to target deep learning-based NIR-powered human detection systems.
              </td>
            </tr>	

            


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2><a href="./index_full.html" style="font-size: 20px;">Full List</a></h2>
                </td>
              </tr>
            </tbody></table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Interns</h2>
                <p>
                <em>2024.08 - 2025.07</em>: Computer Vision Research Intern at Shanghai AI Lab, mentored by <a href="https://zzh-tech.github.io/">Zhihang Zhong</a> and <a href="https://jimmysuen.github.io/">Xiao Sun</a>.
                <br>
                <em>2023.12 - 2024.06</em>: Computer Vision Research Intern at Computer Vision Center (CVC), Tencent AI Lab, mentored by <a href="http://vinthony.github.io">Xiaodong Cun</a>.
                <br>
                <em>2021.08 - 2022.01</em>: Computer Vision Research Intern at SenseVideo Group, SenseTime Research, mentored by Siwei Tang.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Awards/Scholarships</h2>
              <p>
                BOOST NAIS Special Research Scholarship (3,900,000 JPY / year), The University of Tokyo, <em>2024.10 - 2027.09</em> <br>
                WING-CFS Special Research Assistant Scholarship (2,160,000 JPY / year), The University of Tokyo, <em>2023.04 - 2024.09</em> <br>
                Teijin Scholarship, <em>2023.04 - 2024.09</em> <br>
                JASSO Scholarship, <em>2022.10 - 2023.04</em> <br>
                National Scholarship of China (Undergraduate Students), <em>2019.09 - 2021.08</em> <br>
              </p>
            </td>
          </tr>
        </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=g-9X4bkMwGxMP6ZUofQvykKl_zZabaIz2JT2h2Wcxwg&cl=ffffff&w=300"></script>

    <!-- <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0"> -->
    <!-- </tr> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  The template comes from the personal website of <a href="https://jonbarron.info/">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>


